{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "file_path =  '../resources/raw_input_data/The_critique_of_pure_reason.txt'\n",
    "\n",
    "\n",
    "f = open(file_path,\"r\")\n",
    "text = f.read()\n",
    "print(text)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "# Just tokenise text and add POS Tags\n",
    "\n",
    "sent = preprocess(text)\n",
    "sent"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "source": [
    "my_string = \",\".join(str(element) for element in sent)\n",
    "my_string\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "source": [
    "pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "# used to chunck parts of sentences :) \n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "source": [
    "cp = nltk.RegexpParser(pattern)\n",
    "tree_cs = cp.parse(sent)\n",
    "print(tree_cs)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import nltk\n",
    "s = '(ROOT (S (NP (NNP Europe)) (VP (VBZ is) (PP (IN in) (NP (DT the) (JJ same) (NNS trends)))) (. .)))'\n",
    "tree = nltk.tree.Tree.fromstring(s)\n",
    "\n",
    "list_entity_relation_match = []\n",
    "\n",
    "def traverse_tree(tree,pointer:list):\n",
    "    list_valid_types = [\"NNP\",\"NNS\"]\n",
    "\n",
    "    for subtree in tree:\n",
    "        if type(subtree) == nltk.tree.Tree:\n",
    "\n",
    "            if subtree.label() in list_valid_types:\n",
    "                pointer.append(subtree.pop()) \n",
    "                \n",
    "            else:\n",
    "                traverse_tree(subtree,pointer)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "def extract_noun_relationships(sentence: str) -> list:\n",
    "    noun_list = []\n",
    "    ER_list = []\n",
    "    word_tree = nltk.tree.Tree.fromstring(sentence)        \n",
    "    traverse_tree(tree,noun_list)\n",
    "    ER_list.extend([noun_list, sentence])\n",
    "    return ER_list\n",
    "\n",
    "\n",
    "# What to do with multiple nouns? \n",
    "\n",
    "\n",
    "extract_noun_relationships(s)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from pprint import pprint\n",
    "iob_tagged = tree2conlltags(cs)\n",
    "pprint(iob_tagged)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "ne_tree = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "print((ne_tree))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "type(iob_tagged)\n",
    "df = pd.DataFrame.from_records(iob_tagged)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "df"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
